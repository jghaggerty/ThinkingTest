{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Bias Diagnostic API - Analysis Workflow\n",
    "\n",
    "This notebook demonstrates how to use the AI Bias Diagnostic API for comprehensive analysis workflows.\n",
    "\n",
    "## Contents\n",
    "1. Setup and Authentication\n",
    "2. Basic API Operations\n",
    "3. Data Collection and Analysis\n",
    "4. Visualization\n",
    "5. Trend Analysis\n",
    "6. Custom Reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import List, Dict\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure API settings\n",
    "BASE_URL = \"http://localhost:8000\"\n",
    "\n",
    "# Simple API client class\n",
    "class BiasAPI:\n",
    "    def __init__(self, base_url: str = BASE_URL):\n",
    "        self.base_url = base_url\n",
    "        self.session = requests.Session()\n",
    "    \n",
    "    def create_evaluation(self, ai_system_name: str, heuristic_types: List[str], \n",
    "                         iteration_count: int = 50) -> Dict:\n",
    "        url = f\"{self.base_url}/api/evaluations\"\n",
    "        payload = {\n",
    "            \"ai_system_name\": ai_system_name,\n",
    "            \"heuristic_types\": heuristic_types,\n",
    "            \"iteration_count\": iteration_count\n",
    "        }\n",
    "        response = self.session.post(url, json=payload)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    \n",
    "    def execute_evaluation(self, evaluation_id: str) -> Dict:\n",
    "        url = f\"{self.base_url}/api/evaluations/{evaluation_id}/execute\"\n",
    "        response = self.session.post(url)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    \n",
    "    def get_evaluation(self, evaluation_id: str) -> Dict:\n",
    "        url = f\"{self.base_url}/api/evaluations/{evaluation_id}\"\n",
    "        response = self.session.get(url)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    \n",
    "    def list_evaluations(self, limit: int = 100, offset: int = 0) -> Dict:\n",
    "        url = f\"{self.base_url}/api/evaluations\"\n",
    "        params = {\"limit\": limit, \"offset\": offset}\n",
    "        response = self.session.get(url, params=params)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    \n",
    "    def get_heuristics(self, evaluation_id: str) -> List[Dict]:\n",
    "        url = f\"{self.base_url}/api/evaluations/{evaluation_id}/heuristics\"\n",
    "        response = self.session.get(url)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    \n",
    "    def get_recommendations(self, evaluation_id: str, mode: str = \"both\") -> Dict:\n",
    "        url = f\"{self.base_url}/api/evaluations/{evaluation_id}/recommendations\"\n",
    "        params = {\"mode\": mode}\n",
    "        response = self.session.get(url, params=params)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "\n",
    "# Initialize API client\n",
    "api = BiasAPI()\n",
    "print(\"✓ API client initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic API Operations\n",
    "\n",
    "Let's create a sample evaluation and explore the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new evaluation\n",
    "evaluation = api.create_evaluation(\n",
    "    ai_system_name=\"Sample AI System - Notebook Demo\",\n",
    "    heuristic_types=[\"anchoring\", \"loss_aversion\", \"confirmation_bias\"],\n",
    "    iteration_count=60\n",
    ")\n",
    "\n",
    "evaluation_id = evaluation['id']\n",
    "print(f\"Created evaluation: {evaluation_id}\")\n",
    "print(f\"System: {evaluation['ai_system_name']}\")\n",
    "print(f\"Status: {evaluation['status']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the evaluation\n",
    "result = api.execute_evaluation(evaluation_id)\n",
    "\n",
    "print(\"Evaluation completed!\")\n",
    "print(f\"Overall Score: {result['overall_score']:.2f}\")\n",
    "print(f\"Zone Status: {result['zone_status']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get detailed findings\n",
    "findings = api.get_heuristics(evaluation_id)\n",
    "\n",
    "# Convert to DataFrame for easier analysis\n",
    "findings_df = pd.DataFrame(findings)\n",
    "findings_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Collection and Analysis\n",
    "\n",
    "Let's collect data from multiple evaluations for comprehensive analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multiple evaluations for different systems\n",
    "test_systems = [\n",
    "    {\n",
    "        \"name\": \"Customer Service Bot\",\n",
    "        \"heuristics\": [\"anchoring\", \"availability_heuristic\"]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Financial Advisor AI\",\n",
    "        \"heuristics\": [\"loss_aversion\", \"sunk_cost\"]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Content Moderator\",\n",
    "        \"heuristics\": [\"confirmation_bias\", \"availability_heuristic\"]\n",
    "    }\n",
    "]\n",
    "\n",
    "evaluation_results = []\n",
    "\n",
    "for system in test_systems:\n",
    "    # Create evaluation\n",
    "    eval_data = api.create_evaluation(\n",
    "        ai_system_name=system['name'],\n",
    "        heuristic_types=system['heuristics'],\n",
    "        iteration_count=50\n",
    "    )\n",
    "    \n",
    "    # Execute\n",
    "    result = api.execute_evaluation(eval_data['id'])\n",
    "    \n",
    "    # Store results\n",
    "    evaluation_results.append({\n",
    "        'id': result['id'],\n",
    "        'system_name': result['ai_system_name'],\n",
    "        'overall_score': result['overall_score'],\n",
    "        'zone_status': result['zone_status'],\n",
    "        'heuristic_count': len(system['heuristics'])\n",
    "    })\n",
    "    \n",
    "    print(f\"✓ {system['name']}: {result['overall_score']:.2f} ({result['zone_status']})\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "results_df = pd.DataFrame(evaluation_results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all findings\n",
    "all_findings = []\n",
    "\n",
    "for eval_result in evaluation_results:\n",
    "    findings = api.get_heuristics(eval_result['id'])\n",
    "    for finding in findings:\n",
    "        finding['system_name'] = eval_result['system_name']\n",
    "        all_findings.append(finding)\n",
    "\n",
    "findings_full_df = pd.DataFrame(all_findings)\n",
    "print(f\"Collected {len(findings_full_df)} findings across {len(evaluation_results)} systems\")\n",
    "findings_full_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualization\n",
    "\n",
    "Create visualizations to understand bias patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall scores comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Define colors for zones\n",
    "zone_colors = {\n",
    "    'green': '#22c55e',\n",
    "    'yellow': '#eab308',\n",
    "    'red': '#ef4444'\n",
    "}\n",
    "\n",
    "colors = [zone_colors[zone] for zone in results_df['zone_status']]\n",
    "\n",
    "plt.bar(results_df['system_name'], results_df['overall_score'], color=colors, alpha=0.7)\n",
    "plt.axhline(y=40, color='green', linestyle='--', label='Green Zone Threshold', alpha=0.5)\n",
    "plt.axhline(y=60, color='orange', linestyle='--', label='Yellow Zone Threshold', alpha=0.5)\n",
    "\n",
    "plt.xlabel('AI System')\n",
    "plt.ylabel('Overall Bias Score')\n",
    "plt.title('Bias Scores by AI System')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heuristic severity comparison\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "pivot_data = findings_full_df.pivot_table(\n",
    "    values='severity_score',\n",
    "    index='heuristic_type',\n",
    "    columns='system_name',\n",
    "    aggfunc='mean'\n",
    ")\n",
    "\n",
    "pivot_data.plot(kind='bar', figsize=(14, 6))\n",
    "plt.xlabel('Heuristic Type')\n",
    "plt.ylabel('Severity Score')\n",
    "plt.title('Heuristic Severity Scores by System')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend(title='System', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Severity vs Confidence scatter plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for system in findings_full_df['system_name'].unique():\n",
    "    system_data = findings_full_df[findings_full_df['system_name'] == system]\n",
    "    plt.scatter(\n",
    "        system_data['severity_score'],\n",
    "        system_data['confidence_level'],\n",
    "        label=system,\n",
    "        s=100,\n",
    "        alpha=0.6\n",
    "    )\n",
    "\n",
    "plt.xlabel('Severity Score')\n",
    "plt.ylabel('Confidence Level')\n",
    "plt.title('Bias Severity vs Detection Confidence')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of findings\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "heatmap_data = findings_full_df.pivot_table(\n",
    "    values='severity_score',\n",
    "    index='system_name',\n",
    "    columns='heuristic_type',\n",
    "    aggfunc='mean',\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "sns.heatmap(\n",
    "    heatmap_data,\n",
    "    annot=True,\n",
    "    fmt='.1f',\n",
    "    cmap='YlOrRd',\n",
    "    cbar_kws={'label': 'Severity Score'}\n",
    ")\n",
    "\n",
    "plt.title('Bias Heatmap: System vs Heuristic Type')\n",
    "plt.xlabel('Heuristic Type')\n",
    "plt.ylabel('AI System')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"Overall Score Statistics:\")\n",
    "print(\"=\" * 50)\n",
    "print(results_df['overall_score'].describe())\n",
    "\n",
    "print(\"\\n\\nSeverity Score Statistics by Heuristic:\")\n",
    "print(\"=\" * 50)\n",
    "print(findings_full_df.groupby('heuristic_type')['severity_score'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis\n",
    "print(\"Correlation between Detection Count and Severity:\")\n",
    "correlation = findings_full_df['detection_count'].corr(\n",
    "    findings_full_df['severity_score']\n",
    ")\n",
    "print(f\"Correlation coefficient: {correlation:.3f}\")\n",
    "\n",
    "# Plot correlation\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(\n",
    "    findings_full_df['detection_count'],\n",
    "    findings_full_df['severity_score'],\n",
    "    alpha=0.5\n",
    ")\n",
    "plt.xlabel('Detection Count')\n",
    "plt.ylabel('Severity Score')\n",
    "plt.title('Detection Count vs Severity Score')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Recommendations Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get recommendations for all evaluations\n",
    "all_recommendations = []\n",
    "\n",
    "for eval_result in evaluation_results:\n",
    "    recs = api.get_recommendations(eval_result['id'], mode=\"both\")\n",
    "    for rec in recs['recommendations']:\n",
    "        rec['system_name'] = eval_result['system_name']\n",
    "        all_recommendations.append(rec)\n",
    "\n",
    "recs_df = pd.DataFrame(all_recommendations)\n",
    "print(f\"Total recommendations: {len(recs_df)}\")\n",
    "recs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze recommendation priorities\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "priority_counts = recs_df.groupby(['system_name', 'estimated_impact']).size().unstack(fill_value=0)\n",
    "priority_counts.plot(kind='bar', stacked=True, figsize=(12, 6))\n",
    "\n",
    "plt.xlabel('AI System')\n",
    "plt.ylabel('Number of Recommendations')\n",
    "plt.title('Recommendations by System and Impact Level')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend(title='Impact Level')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation difficulty analysis\n",
    "difficulty_impact = recs_df.groupby(['implementation_difficulty', 'estimated_impact']).size().unstack(fill_value=0)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "difficulty_impact.plot(kind='bar', figsize=(10, 6))\n",
    "plt.xlabel('Implementation Difficulty')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Implementation Difficulty vs Estimated Impact')\n",
    "plt.legend(title='Impact Level')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Custom Report Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary_report(results_df, findings_df, recs_df):\n",
    "    \"\"\"\n",
    "    Generate a comprehensive summary report.\n",
    "    \"\"\"\n",
    "    report = []\n",
    "    report.append(\"=\" * 80)\n",
    "    report.append(\"AI BIAS DIAGNOSTIC - SUMMARY REPORT\")\n",
    "    report.append(\"=\" * 80)\n",
    "    report.append(f\"\\nGenerated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    report.append(f\"Total Systems Evaluated: {len(results_df)}\")\n",
    "    report.append(f\"Total Bias Patterns Detected: {len(findings_df)}\")\n",
    "    report.append(f\"Total Recommendations: {len(recs_df)}\")\n",
    "    \n",
    "    # Zone distribution\n",
    "    report.append(\"\\n\" + \"-\" * 80)\n",
    "    report.append(\"ZONE DISTRIBUTION\")\n",
    "    report.append(\"-\" * 80)\n",
    "    zone_counts = results_df['zone_status'].value_counts()\n",
    "    for zone, count in zone_counts.items():\n",
    "        pct = (count / len(results_df)) * 100\n",
    "        report.append(f\"{zone.upper():10s}: {count:2d} systems ({pct:5.1f}%)\")\n",
    "    \n",
    "    # Top concerns\n",
    "    report.append(\"\\n\" + \"-\" * 80)\n",
    "    report.append(\"TOP CONCERNS\")\n",
    "    report.append(\"-\" * 80)\n",
    "    worst_systems = results_df.nlargest(3, 'overall_score')\n",
    "    for idx, system in worst_systems.iterrows():\n",
    "        report.append(f\"\\n• {system['system_name']}\")\n",
    "        report.append(f\"  Score: {system['overall_score']:.2f} ({system['zone_status']})\")\n",
    "    \n",
    "    # Most common biases\n",
    "    report.append(\"\\n\" + \"-\" * 80)\n",
    "    report.append(\"MOST PREVALENT BIASES\")\n",
    "    report.append(\"-\" * 80)\n",
    "    bias_severity = findings_df.groupby('heuristic_type')['severity_score'].mean().sort_values(ascending=False)\n",
    "    for heuristic, severity in bias_severity.head(5).items():\n",
    "        report.append(f\"• {heuristic.replace('_', ' ').title():30s}: {severity:5.2f}\")\n",
    "    \n",
    "    # High-priority recommendations\n",
    "    report.append(\"\\n\" + \"-\" * 80)\n",
    "    report.append(\"HIGH-PRIORITY ACTIONS\")\n",
    "    report.append(\"-\" * 80)\n",
    "    high_priority = recs_df[\n",
    "        (recs_df['estimated_impact'] == 'high') & \n",
    "        (recs_df['implementation_difficulty'].isin(['easy', 'moderate']))\n",
    "    ].head(5)\n",
    "    \n",
    "    for idx, rec in high_priority.iterrows():\n",
    "        report.append(f\"\\n{rec['priority']}. {rec['action_title']}\")\n",
    "        report.append(f\"   System: {rec['system_name']}\")\n",
    "        report.append(f\"   Impact: {rec['estimated_impact'].upper()} | \"\n",
    "                     f\"Difficulty: {rec['implementation_difficulty'].upper()}\")\n",
    "    \n",
    "    report.append(\"\\n\" + \"=\" * 80)\n",
    "    \n",
    "    return \"\\n\".join(report)\n",
    "\n",
    "# Generate and display report\n",
    "report = generate_summary_report(results_df, findings_full_df, recs_df)\n",
    "print(report)\n",
    "\n",
    "# Save to file\n",
    "with open('bias_analysis_report.txt', 'w') as f:\n",
    "    f.write(report)\n",
    "print(\"\\n✓ Report saved to: bias_analysis_report.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export Data for Further Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export all data to CSV files\n",
    "results_df.to_csv('evaluation_results.csv', index=False)\n",
    "findings_full_df.to_csv('bias_findings.csv', index=False)\n",
    "recs_df.to_csv('recommendations.csv', index=False)\n",
    "\n",
    "print(\"✓ Data exported to CSV files:\")\n",
    "print(\"  - evaluation_results.csv\")\n",
    "print(\"  - bias_findings.csv\")\n",
    "print(\"  - recommendations.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export complete analysis to JSON\n",
    "export_data = {\n",
    "    \"export_timestamp\": datetime.now().isoformat(),\n",
    "    \"summary\": {\n",
    "        \"total_evaluations\": len(results_df),\n",
    "        \"total_findings\": len(findings_full_df),\n",
    "        \"total_recommendations\": len(recs_df),\n",
    "        \"average_score\": float(results_df['overall_score'].mean()),\n",
    "        \"zone_distribution\": results_df['zone_status'].value_counts().to_dict()\n",
    "    },\n",
    "    \"evaluations\": results_df.to_dict('records'),\n",
    "    \"findings\": findings_full_df.to_dict('records'),\n",
    "    \"recommendations\": recs_df.to_dict('records')\n",
    "}\n",
    "\n",
    "with open('complete_analysis.json', 'w') as f:\n",
    "    json.dump(export_data, f, indent=2, default=str)\n",
    "\n",
    "print(\"✓ Complete analysis exported to: complete_analysis.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Basic API Usage**: Creating and executing evaluations\n",
    "2. **Data Collection**: Gathering data from multiple evaluations\n",
    "3. **Visualization**: Creating charts to understand bias patterns\n",
    "4. **Statistical Analysis**: Analyzing correlations and distributions\n",
    "5. **Recommendations**: Understanding and prioritizing actions\n",
    "6. **Reporting**: Generating comprehensive reports\n",
    "7. **Export**: Saving data for further analysis\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Use this workflow for regular monitoring of AI systems\n",
    "- Customize visualizations for your specific needs\n",
    "- Integrate with your existing data science pipelines\n",
    "- Build automated reporting dashboards\n",
    "- Track longitudinal trends over time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
