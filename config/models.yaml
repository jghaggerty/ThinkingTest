# AI Bias Psychologist - LLM Model Configuration
# This file defines available models, their providers, and configuration parameters

providers:
  openai:
    name: "OpenAI"
    base_url: "https://api.openai.com/v1"
    models:
      gpt-4:
        name: "GPT-4"
        max_tokens: 8192
        context_window: 128000
        cost_per_1k_tokens: 0.03
        capabilities: ["text", "reasoning", "analysis"]
        recommended_temperature: 0.7
        rate_limits:
          requests_per_minute: 500
          tokens_per_minute: 150000
      
      gpt-4-turbo:
        name: "GPT-4 Turbo"
        max_tokens: 4096
        context_window: 128000
        cost_per_1k_tokens: 0.01
        capabilities: ["text", "reasoning", "analysis", "fast"]
        recommended_temperature: 0.7
        rate_limits:
          requests_per_minute: 500
          tokens_per_minute: 300000
      
      gpt-3.5-turbo:
        name: "GPT-3.5 Turbo"
        max_tokens: 4096
        context_window: 16384
        cost_per_1k_tokens: 0.002
        capabilities: ["text", "reasoning", "fast", "cost_effective"]
        recommended_temperature: 0.7
        rate_limits:
          requests_per_minute: 3500
          tokens_per_minute: 90000

  anthropic:
    name: "Anthropic"
    base_url: "https://api.anthropic.com"
    models:
      claude-3-opus:
        name: "Claude 3 Opus"
        max_tokens: 4096
        context_window: 200000
        cost_per_1k_tokens: 0.015
        capabilities: ["text", "reasoning", "analysis", "long_context"]
        recommended_temperature: 0.7
        rate_limits:
          requests_per_minute: 5
          tokens_per_minute: 40000
      
      claude-3-sonnet:
        name: "Claude 3 Sonnet"
        max_tokens: 4096
        context_window: 200000
        cost_per_1k_tokens: 0.003
        capabilities: ["text", "reasoning", "analysis", "balanced"]
        recommended_temperature: 0.7
        rate_limits:
          requests_per_minute: 5
          tokens_per_minute: 40000
      
      claude-3-haiku:
        name: "Claude 3 Haiku"
        max_tokens: 4096
        context_window: 200000
        cost_per_1k_tokens: 0.00025
        capabilities: ["text", "reasoning", "fast", "cost_effective"]
        recommended_temperature: 0.7
        rate_limits:
          requests_per_minute: 5
          tokens_per_minute: 40000

  ollama:
    name: "Ollama (Local)"
    base_url: "http://localhost:11434"
    models:
      llama3:
        name: "Llama 3 8B"
        max_tokens: 2048
        context_window: 8192
        cost_per_1k_tokens: 0.0
        capabilities: ["text", "reasoning", "local", "free"]
        recommended_temperature: 0.7
        rate_limits:
          requests_per_minute: 1000
          tokens_per_minute: 10000
        system_requirements:
          min_ram_gb: 8
          recommended_ram_gb: 16
      
      llama3:70b:
        name: "Llama 3 70B"
        max_tokens: 2048
        context_window: 8192
        cost_per_1k_tokens: 0.0
        capabilities: ["text", "reasoning", "local", "free", "high_quality"]
        recommended_temperature: 0.7
        rate_limits:
          requests_per_minute: 100
          tokens_per_minute: 2000
        system_requirements:
          min_ram_gb: 40
          recommended_ram_gb: 64
      
      mistral:
        name: "Mistral 7B"
        max_tokens: 2048
        context_window: 8192
        cost_per_1k_tokens: 0.0
        capabilities: ["text", "reasoning", "local", "free", "efficient"]
        recommended_temperature: 0.7
        rate_limits:
          requests_per_minute: 1000
          tokens_per_minute: 15000
        system_requirements:
          min_ram_gb: 6
          recommended_ram_gb: 12
      
      codellama:
        name: "Code Llama"
        max_tokens: 2048
        context_window: 8192
        cost_per_1k_tokens: 0.0
        capabilities: ["text", "reasoning", "local", "free", "code"]
        recommended_temperature: 0.7
        rate_limits:
          requests_per_minute: 1000
          tokens_per_minute: 10000
        system_requirements:
          min_ram_gb: 8
          recommended_ram_gb: 16

# Model selection presets
presets:
  high_quality:
    description: "Best quality models for research"
    models: ["gpt-4", "claude-3-opus", "llama3:70b"]
    temperature: 0.7
  
  balanced:
    description: "Good quality with reasonable cost"
    models: ["gpt-4-turbo", "claude-3-sonnet", "llama3"]
    temperature: 0.7
  
  cost_effective:
    description: "Lower cost options for large-scale testing"
    models: ["gpt-3.5-turbo", "claude-3-haiku", "mistral"]
    temperature: 0.7
  
  local_only:
    description: "Local models for privacy-sensitive testing"
    models: ["llama3", "llama3:70b", "mistral", "codellama"]
    temperature: 0.7

# Default configurations
defaults:
  provider: "openai"
  model: "gpt-4"
  temperature: 0.7
  max_tokens: 1000
  timeout: 30
  retry_attempts: 3
  retry_delay: 1.0

# API configuration
api_config:
  timeout: 30
  max_retries: 3
  retry_delay: 1.0
  exponential_backoff: true
  rate_limit_buffer: 0.1  # 10% buffer on rate limits
  
# Cost tracking
cost_tracking:
  enabled: true
  currency: "USD"
  budget_limit: 100.0
  alert_threshold: 80.0
