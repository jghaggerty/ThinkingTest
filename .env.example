# AI Bias Psychologist - Environment Configuration Template
# Copy this file to .env and fill in your actual values

# =============================================================================
# APPLICATION SETTINGS
# =============================================================================

# Application name and version
APP_NAME="AI Bias Psychologist"
APP_VERSION="0.1.0"

# Debug mode (true/false)
DEBUG=false

# Log level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
LOG_LEVEL=INFO

# =============================================================================
# DATABASE CONFIGURATION
# =============================================================================

# Database URL (SQLite for development, PostgreSQL for production)
DATABASE_URL="sqlite:///./data/database/bias_psych.db"

# =============================================================================
# STORAGE PATHS
# =============================================================================

# Base data directory
BASE_DIR="./data"

# Specific storage directories
RESPONSES_DIR="./data/responses"
EXPORTS_DIR="./data/exports"
LOGS_DIR="./data/logs"

# =============================================================================
# LLM API KEYS
# =============================================================================

# OpenAI API Configuration
OPENAI_API_KEY="your_openai_api_key_here"
OPENAI_ORG_ID="your_openai_org_id_here"

# Anthropic API Configuration
ANTHROPIC_API_KEY="your_anthropic_api_key_here"

# =============================================================================
# DASHBOARD CONFIGURATION
# =============================================================================

# Dashboard host and port
DASHBOARD_HOST="0.0.0.0"
DASHBOARD_PORT=8000

# CORS origins
CORS_ORIGINS="*"

# =============================================================================
# SECURITY SETTINGS
# =============================================================================

# API key requirement
API_KEY_REQUIRED=false

# Rate limiting
RATE_LIMIT_REQUESTS_PER_MINUTE=60

# =============================================================================
# PERFORMANCE SETTINGS
# =============================================================================

# Maximum concurrent requests
MAX_CONCURRENT_REQUESTS=10

# Request timeout in seconds
REQUEST_TIMEOUT=30

# Response caching
CACHE_RESPONSES=true
CACHE_TTL=3600

# =============================================================================
# ANALYTICS CONFIGURATION
# =============================================================================

# Effect size threshold for bias detection
EFFECT_SIZE_THRESHOLD=0.2

# Statistical significance level
SIGNIFICANCE_LEVEL=0.05

# Confidence interval level
CONFIDENCE_INTERVAL=0.90

# =============================================================================
# FEATURE FLAGS
# =============================================================================

# Enable real-time dashboard
REAL_TIME_DASHBOARD=true

# Enable batch processing
BATCH_PROCESSING=true

# Enable model comparison
MODEL_COMPARISON=true

# Enable trend analysis
TREND_ANALYSIS=true

# Enable export functionality
EXPORT_FUNCTIONALITY=true

# Enable educational content
EDUCATIONAL_CONTENT=true

# Enable bias mitigation recommendations (v0.1 scope: false)
BIAS_MITIGATION_RECOMMENDATIONS=false

# =============================================================================
# DEVELOPMENT SETTINGS
# =============================================================================

# Hot reload for development
HOT_RELOAD=false

# Mock LLM responses for testing
MOCK_LLM_RESPONSES=false

# Test mode
TEST_MODE=false

# =============================================================================
# OLLAMA CONFIGURATION (Local Models)
# =============================================================================

# Ollama base URL
OLLAMA_BASE_URL="http://localhost:11434"

# Default local model
DEFAULT_LOCAL_MODEL="llama3"

# =============================================================================
# ADVANCED SETTINGS
# =============================================================================

# Default temperature for probe execution
DEFAULT_TEMPERATURE=0.7

# Default max tokens for responses
DEFAULT_MAX_TOKENS=1000

# Default timeout for probe execution
DEFAULT_TIMEOUT=30

# Maximum retry attempts
MAX_RETRIES=3

# Retry delay in seconds
RETRY_DELAY=1.0
